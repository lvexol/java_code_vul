{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install javalang\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKXWyGg1ZeJZ",
        "outputId": "44dfb87f-e139-4070-f064-9e1065145b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting javalang\n",
            "  Downloading javalang-0.13.0-py3-none-any.whl.metadata (805 bytes)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from javalang) (1.16.0)\n",
            "Downloading javalang-0.13.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: javalang\n",
            "Successfully installed javalang-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh0hzi-qYvz0",
        "outputId": "fea709de-d881-4140-c064-a7223352ba48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m656/656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.5455 - loss: 0.6794\n",
            "Epoch 2/5\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.6770 - loss: 0.5973\n",
            "Epoch 3/5\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.6020 - loss: 0.6177\n",
            "Epoch 4/5\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.6223 - loss: 0.6088\n",
            "Epoch 5/5\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8774 - loss: 0.4589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a Java code snippet: public void saveAsAuthor(String comment, boolean minorEdit) throws XWikiException     {         XWikiContext xcontext = getXWikiContext();          getAuthors()             .setOriginalMetadataAuthor(getCurrentUserReferenceResolver().resolve(CurrentUserReference.INSTANCE));         DocumentReference author = getEffectiveAuthorReference();         if (hasAccess(Right.EDIT, author)) {             DocumentReference currentUser = xcontext.getUserReference();             try {                 xcontext.setUserReference(author);                  saveDocument(comment, minorEdit);             } finally {                 xcontext.setUserReference(currentUser);             }         } else {             java.lang.Object[] args = { author, xcontext.getDoc(), getFullName() };             throw new XWikiException(XWikiException.MODULE_XWIKI_ACCESS, XWikiException.ERROR_XWIKI_ACCESS_DENIED,                 \"Access denied; user {0}, acting through script in document {1} cannot save document {2}\", null, args);         }     }\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
            "0.95642865\n",
            "The entered code is: Vulnerable\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import javalang\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Load dataset from Excel file\n",
        "df = pd.read_excel('/content/drive/MyDrive/meg2.xlsx')\n",
        "\n",
        "# Function to preprocess code and handle common issues (like unmatched quotes)\n",
        "def preprocess_code(code):\n",
        "    if not isinstance(code, str):\n",
        "        return \"\"\n",
        "    # Replace unmatched single quotes with double quotes\n",
        "    code = re.sub(r\"(?<!\\\\)\\'\", '\"', code)\n",
        "    return code\n",
        "\n",
        "# Tokenize Java code using javalang and ignore errors\n",
        "def tokenize_java_code(code):\n",
        "    try:\n",
        "        # Tokenize and return list of tokens as strings\n",
        "        tokens = list(javalang.tokenizer.tokenize(code, ignore_errors=True))\n",
        "        return [str(token) for token in tokens]\n",
        "    except javalang.tokenizer.LexerError as e:\n",
        "        # Print the error and return an empty list for this row\n",
        "        print(f\"LexerError: {e} for code: {code}\")\n",
        "        return []\n",
        "\n",
        "# Apply preprocessing to the func_before column\n",
        "df['func_before_cleaned'] = df['func_before'].apply(preprocess_code)\n",
        "\n",
        "# Apply tokenization to the cleaned column\n",
        "df['tokenized_func_before'] = df['func_before_cleaned'].apply(tokenize_java_code)\n",
        "\n",
        "# Filter out rows where tokenization failed\n",
        "df = df[df['tokenized_func_before'].apply(lambda x: len(x) > 0)]\n",
        "\n",
        "# Convert tokenized Java code back into text form for Keras Tokenizer\n",
        "X = df['tokenized_func_before'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Convert labels to binary\n",
        "y = df['is_vul'].apply(lambda x: 1 if str(x).strip().upper() == 'TRUE' else 0)\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y),\n",
        "    y=y\n",
        ")\n",
        "\n",
        "# Create class_weight_dict based on the entire dataset\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "# Preprocessing: Use Keras Tokenizer\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_seq = tokenizer.texts_to_sequences(X)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Padding sequences to ensure uniform length\n",
        "maxlen = 200  # Adjust based on typical code length\n",
        "X_pad = pad_sequences(X_seq, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Build and train the model on the full dataset\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, input_length=maxlen),\n",
        "    tf.keras.layers.LSTM(128, return_sequences=False),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_pad, y, epochs=5, batch_size=64, class_weight=class_weight_dict)\n",
        "\n",
        "# Save the model\n",
        "model.save('vul_detection_model.h5')\n",
        "\n",
        "# Function to predict if code is vulnerable\n",
        "def predict_vulnerability(code_snippet):\n",
        "    code_snippet = preprocess_code(code_snippet)  # Preprocess the user input\n",
        "    tokenized_code = ' '.join(tokenize_java_code(code_snippet))  # Tokenize input code\n",
        "    seq = tokenizer.texts_to_sequences([tokenized_code])\n",
        "    padded_seq = pad_sequences(seq, padding='post', maxlen=maxlen)\n",
        "    prediction = model.predict(padded_seq)[0][0]\n",
        "    print(prediction)\n",
        "    return 'Vulnerable' if prediction > 0.5 else 'Not Vulnerable'\n",
        "\n",
        "# Test the model with user input\n",
        "user_code = input(\"Enter a Java code snippet: \")\n",
        "result = predict_vulnerability(user_code)\n",
        "print(f'The entered code is: {result}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_vulnerability(code_snippet):\n",
        "    code_snippet = preprocess_code(code_snippet)  # Preprocess the user input\n",
        "    tokenized_code = ' '.join(tokenize_java_code(code_snippet))  # Tokenize input code\n",
        "    seq = tokenizer.texts_to_sequences([tokenized_code])\n",
        "    padded_seq = pad_sequences(seq, padding='post', maxlen=maxlen)\n",
        "    prediction = model.predict(padded_seq)[0][0]\n",
        "    print(prediction)\n",
        "    return 'Vulnerable' if prediction > 0.5 else 'Not Vulnerable'\n",
        "\n",
        "# Test the model with user input\n",
        "user_code = input(\"Enter a Java code snippet: \")\n",
        "result = predict_vulnerability(user_code)\n",
        "print(f'The entered code is: {result}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pF1mhzVbtVr",
        "outputId": "dd405df3-06d1-4279-d199-9eee63a7f6f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a Java code snippet: public void saveAsAuthor(String comment, boolean minorEdit) throws XWikiException     {         XWikiContext xcontext = getXWikiContext();          getAuthors()             .setOriginalMetadataAuthor(getCurrentUserReferenceResolver().resolve(CurrentUserReference.INSTANCE));         DocumentReference author = getEffectiveAuthorReference();         if (hasAccess(Right.EDIT, author)) {             DocumentReference currentUser = xcontext.getUserReference();             try {                 xcontext.setUserReference(author);                  saveDocument(comment, minorEdit);             } finally {                 xcontext.setUserReference(currentUser);             }         } else {             java.lang.Object[] args = { author, xcontext.getDoc(), getFullName() };             throw new XWikiException(XWikiException.MODULE_XWIKI_ACCESS, XWikiException.ERROR_XWIKI_ACCESS_DENIED,                 \"Access denied; user {0}, acting through script in document {1} cannot save document {2}\", null, args);         }     }\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "0.61995715\n",
            "The entered code is: Vulnerable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR2SEhDSosGW",
        "outputId": "8e1a48cd-1088-404a-fd2f-251b59d75267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}